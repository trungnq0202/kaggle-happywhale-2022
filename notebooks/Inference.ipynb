{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "\n",
    "# For data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Pytorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda import amp\n",
    "\n",
    "# Utils\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# Sklearn Imports\n",
    "from sklearn.preprocessing import LabelEncoder, normalize\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# For Image Models\n",
    "import timm\n",
    "\n",
    "# For Similarity Search\n",
    "import faiss\n",
    "\n",
    "# Albumentations for augmentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# For colored terminal text\n",
    "from colorama import Fore, Back, Style\n",
    "b_ = Fore.BLUE\n",
    "y_ = Fore.YELLOW\n",
    "sr_ = Style.RESET_ALL\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# For descriptive error messages\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"seed\": 3698,\n",
    "    \"img_size\": 448,\n",
    "    \"model_name\": \"tf_efficientnet_b0_ns\",\n",
    "    \"num_classes\": 15587,\n",
    "    \"embedding_size\": 512,\n",
    "    \"train_batch_size\": 64,\n",
    "    \"valid_batch_size\": 64,\n",
    "    \"n_fold\": 5,\n",
    "    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    # ArcFace Hyperparameters\n",
    "    \"s\": 30.0, \n",
    "    \"m\": 0.30,\n",
    "    \"ls_eps\": 0.0,\n",
    "    \"easy_margin\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_dir = '../runs/effnet_arcface_gem/checkpoint_fold4/best_loss.pth'\n",
    "test_folder_dir = '../data/happy-whale-and-dolphin/test_images'\n",
    "\n",
    "train_csv_dir = '../lists/train_modified.csv'\n",
    "test_csv_dir = '../lists/test_modified.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seed for Reducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=3698):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "set_seed(CONFIG['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(train_csv_dir, index_col=0)\n",
    "df_test = pd.read_csv(test_csv_dir, index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>species</th>\n",
       "      <th>individual_id</th>\n",
       "      <th>image_path</th>\n",
       "      <th>split</th>\n",
       "      <th>class</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00021adfb725ed.jpg</td>\n",
       "      <td>melon_headed_whale</td>\n",
       "      <td>12348</td>\n",
       "      <td>../data/happy-whale-and-dolphin/train_images/0...</td>\n",
       "      <td>Train</td>\n",
       "      <td>whale</td>\n",
       "      <td>804</td>\n",
       "      <td>671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000562241d384d.jpg</td>\n",
       "      <td>humpback_whale</td>\n",
       "      <td>1636</td>\n",
       "      <td>../data/happy-whale-and-dolphin/train_images/0...</td>\n",
       "      <td>Train</td>\n",
       "      <td>whale</td>\n",
       "      <td>3504</td>\n",
       "      <td>2336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0007c33415ce37.jpg</td>\n",
       "      <td>false_killer_whale</td>\n",
       "      <td>5842</td>\n",
       "      <td>../data/happy-whale-and-dolphin/train_images/0...</td>\n",
       "      <td>Train</td>\n",
       "      <td>whale</td>\n",
       "      <td>3599</td>\n",
       "      <td>2399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0007d9bca26a99.jpg</td>\n",
       "      <td>bottlenose_dolphin</td>\n",
       "      <td>4551</td>\n",
       "      <td>../data/happy-whale-and-dolphin/train_images/0...</td>\n",
       "      <td>Train</td>\n",
       "      <td>dolphin</td>\n",
       "      <td>3504</td>\n",
       "      <td>2336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00087baf5cef7a.jpg</td>\n",
       "      <td>humpback_whale</td>\n",
       "      <td>8721</td>\n",
       "      <td>../data/happy-whale-and-dolphin/train_images/0...</td>\n",
       "      <td>Train</td>\n",
       "      <td>whale</td>\n",
       "      <td>3599</td>\n",
       "      <td>2699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                image             species  individual_id  \\\n",
       "0  00021adfb725ed.jpg  melon_headed_whale          12348   \n",
       "1  000562241d384d.jpg      humpback_whale           1636   \n",
       "2  0007c33415ce37.jpg  false_killer_whale           5842   \n",
       "3  0007d9bca26a99.jpg  bottlenose_dolphin           4551   \n",
       "4  00087baf5cef7a.jpg      humpback_whale           8721   \n",
       "\n",
       "                                          image_path  split    class  width  \\\n",
       "0  ../data/happy-whale-and-dolphin/train_images/0...  Train    whale    804   \n",
       "1  ../data/happy-whale-and-dolphin/train_images/0...  Train    whale   3504   \n",
       "2  ../data/happy-whale-and-dolphin/train_images/0...  Train    whale   3599   \n",
       "3  ../data/happy-whale-and-dolphin/train_images/0...  Train  dolphin   3504   \n",
       "4  ../data/happy-whale-and-dolphin/train_images/0...  Train    whale   3599   \n",
       "\n",
       "   height  \n",
       "0     671  \n",
       "1    2336  \n",
       "2    2399  \n",
       "3    2336  \n",
       "4    2699  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "\n",
    "with open(\"le.pkl\", \"rb\") as fp:\n",
    "    encoder = joblib.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=CONFIG['n_fold'])\n",
    "\n",
    "for fold, ( _, val_) in enumerate(skf.split(X=df, y=df.individual_id)):\n",
    "    df.loc[val_ , \"kfold\"] = fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HappyWhaleDataset(Dataset):\n",
    "    def __init__(self, df, transforms=None):\n",
    "        self.df = df\n",
    "        self.ids = df['image'].values\n",
    "        self.file_names = df['image_path'].values\n",
    "        self.labels = df['individual_id'].values\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        idx = self.ids[index]\n",
    "        img_path = self.file_names[index]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "            \n",
    "        return {\n",
    "            'image': img,\n",
    "            'label': torch.tensor(label, dtype=torch.long),\n",
    "            'id': idx\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    \"train\" : A.Compose(\n",
    "            [\n",
    "                A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
    "                A.ShiftScaleRotate(\n",
    "                    shift_limit=0.1,\n",
    "                    scale_limit=0.15,\n",
    "                    rotate_limit=60,\n",
    "                    p=0.5\n",
    "                ),\n",
    "                A.HueSaturationValue(\n",
    "                    hue_shift_limit=0.2,\n",
    "                    sat_shift_limit=0.2,\n",
    "                    val_shift_limit=0.2,\n",
    "                    p=0.5\n",
    "                ),\n",
    "                A.RandomBrightnessContrast(\n",
    "                    brightness_limit=(-0.1, 0.1), \n",
    "                    contrast_limit=(-0.1, 0.1),\n",
    "                    p=0.5\n",
    "                ),\n",
    "                A.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225],\n",
    "                    max_pixel_value=255.0,\n",
    "                    p=1.0\n",
    "                ),\n",
    "                ToTensorV2()\n",
    "            ], p=1.),\n",
    "\n",
    "    \"validate\" : A.Compose([\n",
    "            A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
    "            A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "                max_pixel_value=255.0,\n",
    "                p=1.0\n",
    "            ),\n",
    "            ToTensorV2()\n",
    "        ], p=1.)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arcface layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcMarginProduct(nn.Module):\n",
    "    \"\"\"Implement of large margin arc distance: :\n",
    "        Args:\n",
    "            in_features: size of each input sample\n",
    "            out_features: size of each output sample\n",
    "            s: norm of input feature\n",
    "            m: margin\n",
    "            cos(theta + m)\n",
    "        \"\"\"\n",
    "    def __init__(self, in_features, out_features, s=30.0, \n",
    "                 m=0.50, easy_margin=False, ls_eps=0.0):\n",
    "        super(ArcMarginProduct, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.ls_eps = ls_eps  # label smoothing\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        # --------------------------- cos(theta) & phi(theta) ---------------------\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        # --------------------------- convert label to one-hot ---------------------\n",
    "        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n",
    "        # _device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        one_hot = torch.zeros(cosine.size(), device=\"cuda:0\")\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        if self.ls_eps > 0:\n",
    "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n",
    "        # -------------torch.where(out_i = {x_i if condition_i else y_i) ------------\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GeM pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeM(nn.Module):\n",
    "    \"\"\"Credit: https://amaarora.github.io/2020/08/30/gempool.html\"\"\"\n",
    "\n",
    "    def __init__(self, p=3, eps=1e-6):\n",
    "        super(GeM, self).__init__()\n",
    "        self.p = nn.Parameter(torch.ones(1) * p)\n",
    "        self.eps = eps\n",
    "\n",
    "    def gem(self, x, p=3, eps=1e-6):\n",
    "        return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gem(x, p=self.p, eps=self.eps)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \\\n",
    "            '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \\\n",
    "                ', ' + 'eps=' + str(self.eps) + ')'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base EffNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEffnetModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, model_name, embedding_size, num_classes, \n",
    "        s, m, easy_margin, ls_eps, pretrained=True, freeze_backbone=False\n",
    "    ):\n",
    "        super(BaseEffnetModel, self).__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained)\n",
    "        in_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Identity()\n",
    "        self.model.global_pool = nn.Identity()\n",
    "        self.pooling = GeM()\n",
    "        self.embedding = nn.Linear(in_features, embedding_size)\n",
    "        self.fc = ArcMarginProduct(\n",
    "            in_features=embedding_size,\n",
    "            out_features=num_classes,\n",
    "            s=s,\n",
    "            m=m,\n",
    "            easy_margin=easy_margin,\n",
    "            ls_eps=ls_eps\n",
    "        )\n",
    "\n",
    "    def forward(self, images, labels):\n",
    "        features = self.model(images)\n",
    "        pooled_features = self.pooling(features).flatten(1)\n",
    "        embedding = self.embedding(pooled_features)\n",
    "        output = self.fc(embedding, labels)\n",
    "        return output\n",
    "    \n",
    "    def extract(self, images):\n",
    "        features = self.model(images)\n",
    "        pooled_features = self.pooling(features).flatten(1)\n",
    "        embedding = self.embedding(pooled_features)\n",
    "        return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseEffnetModel(\n",
    "    model_name=CONFIG['model_name'],\n",
    "    embedding_size= CONFIG['embedding_size'],\n",
    "    num_classes= CONFIG['num_classes'],\n",
    "    s= CONFIG['s'],\n",
    "    m= CONFIG['m'],\n",
    "    easy_margin= CONFIG['easy_margin'],\n",
    "    ls_eps= CONFIG['ls_eps'],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseEffnetModel(\n",
       "  (model): EfficientNet(\n",
       "    (conv_stem): Conv2dSame(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "    (bn1): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act1): SiLU(inplace=True)\n",
       "    (blocks): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): DepthwiseSeparableConv(\n",
       "          (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn1): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pw): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2dSame(96, 96, kernel_size=(3, 3), stride=(2, 2), groups=96, bias=False)\n",
       "          (bn2): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (bn2): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2dSame(144, 144, kernel_size=(5, 5), stride=(2, 2), groups=144, bias=False)\n",
       "          (bn2): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "          (bn2): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2dSame(240, 240, kernel_size=(3, 3), stride=(2, 2), groups=240, bias=False)\n",
       "          (bn2): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "          (bn2): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "          (bn2): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "          (bn2): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "          (bn2): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "          (bn2): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2dSame(672, 672, kernel_size=(5, 5), stride=(2, 2), groups=672, bias=False)\n",
       "          (bn2): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "          (bn2): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "          (bn2): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): InvertedResidual(\n",
       "          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "          (bn2): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "          (bn2): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_head): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn2): BatchNorm2d(1280, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act2): SiLU(inplace=True)\n",
       "    (global_pool): Identity()\n",
       "    (classifier): Identity()\n",
       "  )\n",
       "  (pooling): GeM(p=3.0039, eps=1e-06)\n",
       "  (embedding): Linear(in_features=1280, out_features=512, bias=True)\n",
       "  (fc): ArcMarginProduct()\n",
       ")"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eff_state = torch.load(model_checkpoint_dir)['model_state_dict']\n",
    "model.load_state_dict(eff_state)\n",
    "model.to(CONFIG['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>species</th>\n",
       "      <th>individual_id</th>\n",
       "      <th>image_path</th>\n",
       "      <th>split</th>\n",
       "      <th>class</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>kfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00021adfb725ed.jpg</td>\n",
       "      <td>melon_headed_whale</td>\n",
       "      <td>12348</td>\n",
       "      <td>../data/happy-whale-and-dolphin/train_images/0...</td>\n",
       "      <td>Train</td>\n",
       "      <td>whale</td>\n",
       "      <td>804</td>\n",
       "      <td>671</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000562241d384d.jpg</td>\n",
       "      <td>humpback_whale</td>\n",
       "      <td>1636</td>\n",
       "      <td>../data/happy-whale-and-dolphin/train_images/0...</td>\n",
       "      <td>Train</td>\n",
       "      <td>whale</td>\n",
       "      <td>3504</td>\n",
       "      <td>2336</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0007c33415ce37.jpg</td>\n",
       "      <td>false_killer_whale</td>\n",
       "      <td>5842</td>\n",
       "      <td>../data/happy-whale-and-dolphin/train_images/0...</td>\n",
       "      <td>Train</td>\n",
       "      <td>whale</td>\n",
       "      <td>3599</td>\n",
       "      <td>2399</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0007d9bca26a99.jpg</td>\n",
       "      <td>bottlenose_dolphin</td>\n",
       "      <td>4551</td>\n",
       "      <td>../data/happy-whale-and-dolphin/train_images/0...</td>\n",
       "      <td>Train</td>\n",
       "      <td>dolphin</td>\n",
       "      <td>3504</td>\n",
       "      <td>2336</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00087baf5cef7a.jpg</td>\n",
       "      <td>humpback_whale</td>\n",
       "      <td>8721</td>\n",
       "      <td>../data/happy-whale-and-dolphin/train_images/0...</td>\n",
       "      <td>Train</td>\n",
       "      <td>whale</td>\n",
       "      <td>3599</td>\n",
       "      <td>2699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                image             species  individual_id  \\\n",
       "0  00021adfb725ed.jpg  melon_headed_whale          12348   \n",
       "1  000562241d384d.jpg      humpback_whale           1636   \n",
       "2  0007c33415ce37.jpg  false_killer_whale           5842   \n",
       "3  0007d9bca26a99.jpg  bottlenose_dolphin           4551   \n",
       "4  00087baf5cef7a.jpg      humpback_whale           8721   \n",
       "\n",
       "                                          image_path  split    class  width  \\\n",
       "0  ../data/happy-whale-and-dolphin/train_images/0...  Train    whale    804   \n",
       "1  ../data/happy-whale-and-dolphin/train_images/0...  Train    whale   3504   \n",
       "2  ../data/happy-whale-and-dolphin/train_images/0...  Train    whale   3599   \n",
       "3  ../data/happy-whale-and-dolphin/train_images/0...  Train  dolphin   3504   \n",
       "4  ../data/happy-whale-and-dolphin/train_images/0...  Train    whale   3599   \n",
       "\n",
       "   height  kfold  \n",
       "0     671    0.0  \n",
       "1    2336    1.0  \n",
       "2    2399    0.0  \n",
       "3    2336    0.0  \n",
       "4    2699    0.0  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def get_embeddings(model, dataloader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    LABELS = []\n",
    "    EMBEDS = []\n",
    "    IDS = []\n",
    "    \n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for step, data in bar:        \n",
    "        images = data['image'].to(device, dtype=torch.float)\n",
    "        labels = data['label'].to(device, dtype=torch.long)\n",
    "        ids = data['id']\n",
    "\n",
    "        outputs = model.extract(images)\n",
    "        \n",
    "        LABELS.append(labels.cpu().numpy())\n",
    "        EMBEDS.append(outputs.cpu().numpy())\n",
    "        IDS.append(ids)\n",
    "    \n",
    "    EMBEDS = np.vstack(EMBEDS)\n",
    "    LABELS = np.concatenate(LABELS)\n",
    "    IDS = np.concatenate(IDS)\n",
    "    \n",
    "    return EMBEDS, LABELS, IDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_loaders(df, fold):\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "    \n",
    "    train_dataset = HappyWhaleDataset(df_train, transforms=data_transforms[\"train\"])\n",
    "    valid_dataset = HappyWhaleDataset(df_valid, transforms=data_transforms[\"validate\"])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], \n",
    "                              num_workers=2, shuffle=False, pin_memory=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], \n",
    "                              num_workers=2, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader = prepare_loaders(df, fold=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|██████████| 638/638 [16:45<00:00,  1.58s/it]\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|██████████| 160/160 [04:12<00:00,  1.58s/it]\n"
     ]
    }
   ],
   "source": [
    "train_embeds, train_labels, train_ids = get_embeddings(model, train_loader, CONFIG['device'])\n",
    "valid_embeds, valid_labels, valid_ids = get_embeddings(model, valid_loader, CONFIG['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeds = normalize(train_embeds, axis=1, norm='l2')\n",
    "valid_embeds = normalize(valid_embeds, axis=1, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = encoder.inverse_transform(train_labels)\n",
    "valid_labels = encoder.inverse_transform(valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatIP(CONFIG['embedding_size'])\n",
    "index.add(train_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, I = index.search(valid_embeds, k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_targets = np.unique(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "new_individual    1850\n",
       "37c7aba965a5        80\n",
       "114207cab555        34\n",
       "a6e325d8e924        31\n",
       "19fbb960f07d        31\n",
       "                  ... \n",
       "c511fbe2acd1         1\n",
       "b90f72a6be9b         1\n",
       "579a23a02325         1\n",
       "045ca1b5a580         1\n",
       "26145086bca6         1\n",
       "Name: target, Length: 4012, dtype: int64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_targets_df = pd.DataFrame(np.stack([valid_ids, valid_labels], axis=1), columns=['image','target'])\n",
    "val_targets_df.loc[~val_targets_df.target.isin(allowed_targets), 'target'] = 'new_individual'\n",
    "val_targets_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10207it [00:13, 737.45it/s] \n"
     ]
    }
   ],
   "source": [
    "valid_df = []\n",
    "for i, val_id in tqdm(enumerate(valid_ids)):\n",
    "    targets = train_labels[I[i]]\n",
    "    distances = D[i]\n",
    "    subset_preds = pd.DataFrame(np.stack([targets,distances],axis=1),columns=['target','distances'])\n",
    "    subset_preds['image'] = val_id\n",
    "    valid_df.append(subset_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>target</th>\n",
       "      <th>distances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00021adfb725ed.jpg</td>\n",
       "      <td>01981b6e8ac9</td>\n",
       "      <td>0.995767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00021adfb725ed.jpg</td>\n",
       "      <td>0b6c7058e353</td>\n",
       "      <td>0.995200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00021adfb725ed.jpg</td>\n",
       "      <td>0c8017782d1f</td>\n",
       "      <td>0.994356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00021adfb725ed.jpg</td>\n",
       "      <td>0d4afee74fa8</td>\n",
       "      <td>0.993907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00021adfb725ed.jpg</td>\n",
       "      <td>111a78cd63bf</td>\n",
       "      <td>0.993708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                image        target  distances\n",
       "0  00021adfb725ed.jpg  01981b6e8ac9   0.995767\n",
       "1  00021adfb725ed.jpg  0b6c7058e353   0.995200\n",
       "2  00021adfb725ed.jpg  0c8017782d1f   0.994356\n",
       "3  00021adfb725ed.jpg  0d4afee74fa8   0.993907\n",
       "4  00021adfb725ed.jpg  111a78cd63bf   0.993708"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df = pd.concat(valid_df).reset_index(drop=True)\n",
    "valid_df = valid_df.groupby(['image','target']).distances.max().reset_index()\n",
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = valid_df.sort_values('distances', ascending=False).reset_index(drop=True)\n",
    "valid_df.to_csv('val_neighbors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_list = ['938b7e931166', '5bf17305f073', '7593d2aee842', '7362d7a01d00','956562ff2888']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(test_df, threshold=0.2):\n",
    "    predictions = {}\n",
    "    for i, row in tqdm(test_df.iterrows()):\n",
    "        if row.image in predictions:\n",
    "            if len(predictions[row.image]) == 5:\n",
    "                continue\n",
    "            predictions[row.image].append(row.target)\n",
    "        elif row.distances > threshold:\n",
    "            predictions[row.image] = [row.target, 'new_individual']\n",
    "        else:\n",
    "            predictions[row.image] = ['new_individual', row.target]\n",
    "\n",
    "    for x in tqdm(predictions):\n",
    "        if len(predictions[x]) < 5:\n",
    "            remaining = [y for y in sample_list if y not in predictions]\n",
    "            predictions[x] = predictions[x] + remaining\n",
    "            predictions[x] = predictions[x][:5]\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_per_image(label, predictions):\n",
    "    try:\n",
    "        return 1 / (predictions[:5].index(label) + 1)\n",
    "    except ValueError:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "398126it [00:15, 26189.60it/s]\n",
      "100%|██████████| 10207/10207 [00:00<00:00, 3202518.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV at threshold 0.0: 0.20625224519120874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "398126it [00:15, 26063.33it/s]\n",
      "100%|██████████| 10207/10207 [00:00<00:00, 3079946.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV at threshold 0.1: 0.20625224519120874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "398126it [00:15, 25140.47it/s]\n",
      "100%|██████████| 10207/10207 [00:00<00:00, 2635836.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV at threshold 0.2: 0.20625224519120874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "398126it [00:15, 26021.30it/s]\n",
      "100%|██████████| 10207/10207 [00:00<00:00, 3105190.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV at threshold 0.30000000000000004: 0.20625224519120874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "398126it [00:15, 25998.17it/s]\n",
      "100%|██████████| 10207/10207 [00:00<00:00, 3179211.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV at threshold 0.4: 0.20625224519120874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "398126it [00:15, 26266.34it/s]\n",
      "100%|██████████| 10207/10207 [00:00<00:00, 3157871.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV at threshold 0.5: 0.20625224519120874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "398126it [00:15, 26337.76it/s]\n",
      "100%|██████████| 10207/10207 [00:00<00:00, 3161369.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV at threshold 0.6000000000000001: 0.20625224519120874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "398126it [00:15, 25784.04it/s]\n",
      "100%|██████████| 10207/10207 [00:00<00:00, 3285087.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV at threshold 0.7000000000000001: 0.20625224519120874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "398126it [00:15, 25764.13it/s]\n",
      "100%|██████████| 10207/10207 [00:00<00:00, 2423919.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV at threshold 0.8: 0.20625224519120874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "398126it [00:15, 26247.41it/s]\n",
      "100%|██████████| 10207/10207 [00:00<00:00, 3268284.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV at threshold 0.9: 0.20625224519120874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "398126it [00:14, 26621.84it/s]\n",
      "100%|██████████| 10207/10207 [00:00<00:00, 2946607.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV at threshold 1.0: 0.2506335521374223\n"
     ]
    }
   ],
   "source": [
    "best_th = 0\n",
    "best_cv = 0\n",
    "for th in [0.1*x for x in range(11)]:\n",
    "    all_preds = get_predictions(valid_df, threshold=th)\n",
    "    cv = 0\n",
    "    for i,row in val_targets_df.iterrows():\n",
    "        target = row.target\n",
    "        preds = all_preds[row.image]\n",
    "        val_targets_df.loc[i,th] = map_per_image(target, preds)\n",
    "    cv = val_targets_df[th].mean()\n",
    "    print(f\"CV at threshold {th}: {cv}\")\n",
    "    if cv > best_cv:\n",
    "        best_th = th\n",
    "        best_cv = cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold 1.0\n",
      "Best cv 0.2506335521374223\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.30000000000000004</th>\n",
       "      <th>0.4</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.6000000000000001</th>\n",
       "      <th>0.7000000000000001</th>\n",
       "      <th>0.8</th>\n",
       "      <th>0.9</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10207.000000</td>\n",
       "      <td>10207.000000</td>\n",
       "      <td>10207.000000</td>\n",
       "      <td>10207.000000</td>\n",
       "      <td>10207.000000</td>\n",
       "      <td>10207.000000</td>\n",
       "      <td>10207.000000</td>\n",
       "      <td>10207.000000</td>\n",
       "      <td>10207.000000</td>\n",
       "      <td>10207.000000</td>\n",
       "      <td>10207.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.206252</td>\n",
       "      <td>0.206252</td>\n",
       "      <td>0.206252</td>\n",
       "      <td>0.206252</td>\n",
       "      <td>0.206252</td>\n",
       "      <td>0.206252</td>\n",
       "      <td>0.206252</td>\n",
       "      <td>0.206252</td>\n",
       "      <td>0.206252</td>\n",
       "      <td>0.206252</td>\n",
       "      <td>0.250634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.319175</td>\n",
       "      <td>0.319175</td>\n",
       "      <td>0.319175</td>\n",
       "      <td>0.319175</td>\n",
       "      <td>0.319175</td>\n",
       "      <td>0.319175</td>\n",
       "      <td>0.319175</td>\n",
       "      <td>0.319175</td>\n",
       "      <td>0.319175</td>\n",
       "      <td>0.319175</td>\n",
       "      <td>0.384931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0.0           0.1           0.2  0.30000000000000004  \\\n",
       "count  10207.000000  10207.000000  10207.000000         10207.000000   \n",
       "mean       0.206252      0.206252      0.206252             0.206252   \n",
       "std        0.319175      0.319175      0.319175             0.319175   \n",
       "min        0.000000      0.000000      0.000000             0.000000   \n",
       "25%        0.000000      0.000000      0.000000             0.000000   \n",
       "50%        0.000000      0.000000      0.000000             0.000000   \n",
       "75%        0.500000      0.500000      0.500000             0.500000   \n",
       "max        1.000000      1.000000      1.000000             1.000000   \n",
       "\n",
       "                0.4           0.5  0.6000000000000001  0.7000000000000001  \\\n",
       "count  10207.000000  10207.000000        10207.000000        10207.000000   \n",
       "mean       0.206252      0.206252            0.206252            0.206252   \n",
       "std        0.319175      0.319175            0.319175            0.319175   \n",
       "min        0.000000      0.000000            0.000000            0.000000   \n",
       "25%        0.000000      0.000000            0.000000            0.000000   \n",
       "50%        0.000000      0.000000            0.000000            0.000000   \n",
       "75%        0.500000      0.500000            0.500000            0.500000   \n",
       "max        1.000000      1.000000            1.000000            1.000000   \n",
       "\n",
       "                0.8           0.9           1.0  \n",
       "count  10207.000000  10207.000000  10207.000000  \n",
       "mean       0.206252      0.206252      0.250634  \n",
       "std        0.319175      0.319175      0.384931  \n",
       "min        0.000000      0.000000      0.000000  \n",
       "25%        0.000000      0.000000      0.000000  \n",
       "50%        0.000000      0.000000      0.000000  \n",
       "75%        0.500000      0.500000      0.500000  \n",
       "max        1.000000      1.000000      1.000000  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Best threshold\", best_th)\n",
    "print(\"Best cv\", best_cv)\n",
    "val_targets_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{False: 8357, True: 1850}\n",
      "best_threshold 0.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>is_new_individual</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "      <th>adjusted_cv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.141225</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.177102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.1</th>\n",
       "      <td>0.141225</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.177102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.2</th>\n",
       "      <td>0.141225</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.177102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.30000000000000004</th>\n",
       "      <td>0.141225</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.177102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.4</th>\n",
       "      <td>0.141225</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.177102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>0.141225</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.177102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.6000000000000001</th>\n",
       "      <td>0.141225</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.177102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.7000000000000001</th>\n",
       "      <td>0.141225</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.177102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.8</th>\n",
       "      <td>0.141225</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.177102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.9</th>\n",
       "      <td>0.141225</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.177102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.084745</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.176271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "is_new_individual       False  True  adjusted_cv\n",
       "0.0                  0.141225   0.5     0.177102\n",
       "0.1                  0.141225   0.5     0.177102\n",
       "0.2                  0.141225   0.5     0.177102\n",
       "0.30000000000000004  0.141225   0.5     0.177102\n",
       "0.4                  0.141225   0.5     0.177102\n",
       "0.5                  0.141225   0.5     0.177102\n",
       "0.6000000000000001   0.141225   0.5     0.177102\n",
       "0.7000000000000001   0.141225   0.5     0.177102\n",
       "0.8                  0.141225   0.5     0.177102\n",
       "0.9                  0.141225   0.5     0.177102\n",
       "1.0                  0.084745   1.0     0.176271"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Adjustment: Since Public lb has nearly 10% 'new_individual' (Be Careful for private LB)\n",
    "val_targets_df['is_new_individual'] = val_targets_df.target=='new_individual'\n",
    "print(val_targets_df.is_new_individual.value_counts().to_dict())\n",
    "val_scores = val_targets_df.groupby('is_new_individual').mean().T\n",
    "val_scores['adjusted_cv'] = val_scores[True]*0.1+val_scores[False]*0.9\n",
    "best_threshold_adjusted = val_scores['adjusted_cv'].idxmax()\n",
    "print(\"best_threshold\",best_threshold_adjusted)\n",
    "val_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51033, 512) (51033,)\n"
     ]
    }
   ],
   "source": [
    "train_embeds = np.concatenate([train_embeds, valid_embeds])\n",
    "train_labels = np.concatenate([train_labels, valid_labels])\n",
    "print(train_embeds.shape,train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatIP(CONFIG['embedding_size'])\n",
    "index.add(train_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['individual_id'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = HappyWhaleDataset(df_test, transforms=data_transforms[\"validate\"])\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG['valid_batch_size'], \n",
    "                         num_workers=2, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|██████████| 437/437 [11:21<00:00,  1.56s/it]\n"
     ]
    }
   ],
   "source": [
    "test_embeds, _, test_ids = get_embeddings(model, test_loader, CONFIG['device'])\n",
    "test_embeds = normalize(test_embeds, axis=1, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, I = index.search(test_embeds, k=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27956it [00:10, 2621.56it/s]\n"
     ]
    }
   ],
   "source": [
    "test_df = []\n",
    "for i, test_id in tqdm(enumerate(test_ids)):\n",
    "    targets = train_labels[I[i]]\n",
    "    distances = D[i]\n",
    "    subset_preds = pd.DataFrame(np.stack([targets, distances], axis=1), columns=['target','distances'])\n",
    "    subset_preds['image'] = test_id\n",
    "    test_df.append(subset_preds)\n",
    "    \n",
    "test_df = pd.concat(test_df).reset_index(drop=True)\n",
    "test_df = test_df.groupby(['image','target']).distances.max().reset_index()\n",
    "test_df = test_df.sort_values('distances', ascending=False).reset_index(drop=True)\n",
    "test_df.to_csv('test_neighbors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1114903it [00:44, 25124.89it/s]\n",
      "100%|██████████| 27956/27956 [00:00<00:00, 3047350.76it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6827583d7f8de8.jpg</td>\n",
       "      <td>be315fad6ef4 new_individual 99591df6b695 aeae6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5ee9b577ad8a7c.jpg</td>\n",
       "      <td>12f2a4406d7e new_individual cd720f8127f5 1c496...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3674a3bf79e4e8.jpg</td>\n",
       "      <td>f195c38bcf17 new_individual cc0e0b020a90 0b297...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>db0c734e80a4e5.jpg</td>\n",
       "      <td>2e268c8dbd31 new_individual 7d07bf29721f cd720...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3cbaaacdf49bed.jpg</td>\n",
       "      <td>84a261c0e5cf new_individual cd720f8127f5 78453...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                image                                        predictions\n",
       "0  6827583d7f8de8.jpg  be315fad6ef4 new_individual 99591df6b695 aeae6...\n",
       "1  5ee9b577ad8a7c.jpg  12f2a4406d7e new_individual cd720f8127f5 1c496...\n",
       "2  3674a3bf79e4e8.jpg  f195c38bcf17 new_individual cc0e0b020a90 0b297...\n",
       "3  db0c734e80a4e5.jpg  2e268c8dbd31 new_individual 7d07bf29721f cd720...\n",
       "4  3cbaaacdf49bed.jpg  84a261c0e5cf new_individual cd720f8127f5 78453..."
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = get_predictions(test_df, best_threshold_adjusted)\n",
    "\n",
    "predictions = pd.Series(predictions).reset_index()\n",
    "predictions.columns = ['image','predictions']\n",
    "predictions['predictions'] = predictions['predictions'].apply(lambda x: ' '.join(x))\n",
    "predictions.to_csv('../result/submission.csv',index=False)\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0e580a931a400f132e721006bb6396d3276cef9a251cc36f8aeb1412ba0d1abd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('kaggle-happywhale')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
